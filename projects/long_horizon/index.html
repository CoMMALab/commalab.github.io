<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Long-Horizon Planning | CoMMA Lab @ Purdue </title> <meta name="author" content=" "> <meta name="description" content="The Computational Robot Motion and Autonomy Lab at Purdue University "> <meta name="keywords" content="robotics, motion planning, purdue"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/commie.png?4fa3804fd0d1ce0a571a53ad5458cfab"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://commalab.org/projects/long_horizon/"> <script src="/assets/js/theme.js?ca131c86afeddc68f0e9d3278afbc9b8"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> CoMMA Lab @ Purdue </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/members">Team </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code </a> </li> <li class="nav-item "> <a class="nav-link" href="/join">Join </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Long-Horizon Planning</h1> <p class="post-description"></p> </header> <article> <p>Most tasks faced by a robot require more than a single “action”—for example, to cook a meal, there are many steps in a recipe; in order to navigate a building, a robot must explore many rooms, find feasible unblocked paths, open doors, and so on; construction tasks have many interlocking dependencies (e.g., peg A in hole B before screw C into socket D, etc.).</p> <p>Critical to solving these kinds of tasks is some <em>abstraction</em> over the set of actions a robot can do, allowing methods to reason over what to do at a higher level (<em>i.e.</em>, rather than thinking about joint angles throughout the entire motion, thinking at the level of “pick up object A” and “place object B on object C”). Designing or automatically finding suitable abstractions for a problem (do you assume to know the set of actions the robot can perform, or is this something you must find out?), finding ways of providing <em>feedback</em> through abstraction (<em>e.g.</em>, what happens when you can’t find a motion to pick up an object? Do you never pick up that object again? Is another object blocking it, or is it something else? How do you discover and inform search about this?), and efficiently searching over the infinite combinatorial explosion of options is essential to solving these problems effectively.</p> </article> <br style="clear:both"> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <a name="yan2025vizcoast" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <div> arXiv </div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/viz_coast-480.webp 480w,/assets/img/publication_preview/viz_coast-800.webp 800w,/assets/img/publication_preview/viz_coast-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/viz_coast.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="viz_coast.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Using VLM Reasoning to Constrain Task and Motion Planning</div> <div class="author"> <a href="/members/muyang">Muyang Yan<sup>*</sup></a>, <a href="/members/miras">Miras Mengdibayev<sup>*</sup></a>, <a href="/members/Ardon">Ardon Floros</a>, <a href="https://www.whguo.me/" class="nonmember" rel="external nofollow noopener" target="_blank">Weihang Guo</a>, <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a>, and <a href="/members/kingston">Zachary Kingston</a> </div> <div class="periodical"> </div> <div class="periodical"> Under Review </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.25548" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In task and motion planning, high-level task planning is done over an abstraction of the world to enable efficient search in long-horizon robotics problems. However, the feasibility of these task-level plans relies on the downward refinability of the abstraction into continuous motion. When a domain’s refinability is poor, task-level plans that appear valid may ultimately fail during motion planning, requiring replanning and resulting in slower overall performance. Prior works mitigate this by encoding refinement issues as constraints to prune infeasible task plans. However, these approaches only add constraints upon refinement failure, expending significant search effort on infeasible branches. We propose VIZ-COAST, a method of leveraging the common-sense spatial reasoning of large pretrained Vision-Language Models to identify issues with downward refinement a priori, bypassing the need to fix these failures during planning. Experiments on two challenging TAMP domains show that our approach is able to extract plausible constraints from images and domain descriptions, drastically reducing planning times and, in some cases, eliminating downward refinement failures altogether, generalizing to a diverse range of instances from the broader domain.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">yan2025vizcoast</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using {VLM} Reasoning to Constrain Task and Motion Planning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Muyang and Mengdibayev, Miras and Floros, Ardon and Guo, Weihang and Kavraki, Lydia E. and Kingston, Zachary}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2510.25548}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Under Review}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <a name="chen2025spasm" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <div> arXiv </div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/spasm-480.webp 480w,/assets/img/publication_preview/spasm-800.webp 800w,/assets/img/publication_preview/spasm-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/spasm.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="spasm.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Differentiable Particle Optimization for Fast Sequential Manipulation</div> <div class="author"> <a href="/members/lucas">Lucas Chen</a>, <a href="/members/shrutheesh">Shrutheesh R. Iyer</a>, and <a href="/members/kingston">Zachary Kingston</a> </div> <div class="periodical"> </div> <div class="periodical"> Under Review </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.07674" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=VK8PYsdXNBk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/CoMMALab/SPaSM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="../papers/spasm" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Sequential robot manipulation tasks require finding collision-free trajectories that satisfy geometric constraints across multiple object interactions in potentially high-dimensional configuration spaces. Solving these problems in real-time and at large scales has remained out of reach due to computational requirements. Recently, GPU-based acceleration has shown promising results, but prior methods achieve limited performance due to CPU-GPU data transfer overhead and complex logic that prevents full hardware utilization. To this end, we present SPaSM (Sampling Particle optimization for Sequential Manipulation), a fully GPU-parallelized framework that compiles constraint evaluation, sampling, and gradient-based optimization into optimized CUDA kernels for end-to-end trajectory optimization without CPU coordination. The method consists of a two-stage particle optimization strategy: first solving placement constraints through massively parallel sampling, then lifting solutions to full trajectory optimization in joint space. Unlike hierarchical approaches, SPaSM jointly optimizes object placements and robot trajectories to handle scenarios where motion feasibility constrains placement options. Experimental evaluation on challenging benchmarks demonstrates solution times in the realm of milliseconds with a 100% success rate; a 4000x speedup compared to existing approaches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">chen2025spasm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Differentiable Particle Optimization for Fast Sequential Manipulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Lucas and Iyer, Shrutheesh R. and Kingston, Zachary}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2510.07674}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Under Review}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <a name="guo2025castl" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#daaa00"> <a href="https://ieee-icra.org" rel="external nofollow noopener" target="_blank"> ICRA </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/castl-480.webp 480w,/assets/img/publication_preview/castl-800.webp 800w,/assets/img/publication_preview/castl-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/castl.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="castl.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">CaStL: Constraints as Specifications through LLM Translation for Long-Horizon Task and Motion Planning</div> <div class="author"> <a href="https://www.whguo.me/" class="nonmember" rel="external nofollow noopener" target="_blank">Weihang Guo</a>, <a href="/members/kingston">Zachary Kingston</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2410.22225" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1109/ICRA55743.2025.11127555" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have demonstrated remarkable ability in long-horizon Task and Motion Planning (TAMP) by translating clear and straightforward natural language problems into formal specifications such as the Planning Domain Definition Language (PDDL). However, real-world problems are often ambiguous and involve many complex constraints. In this paper, we introduce Constraints as Specifications through LLMs (CaStL), a framework that identifies constraints such as goal conditions, action ordering, and action blocking from natural language in multiple stages. CaStL translates these constraints into PDDL and Python scripts, which are solved using an custom PDDL solver. Tested across three PDDL domains, CaStL significantly improves constraint handling and planning success rates from natural language specification in complex scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">guo2025castl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{CaStL}: Constraints as Specifications through {LLM} Translation for Long-Horizon Task and Motion Planning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guo, Weihang and Kingston, Zachary and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11957--11964}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA55743.2025.11127555}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <a name="elimelech2024skills" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#daaa00"> <a href="https://ieee-icra.org" rel="external nofollow noopener" target="_blank"> ICRA </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/khen_skills-480.webp 480w,/assets/img/publication_preview/khen_skills-800.webp 800w,/assets/img/publication_preview/khen_skills-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/khen_skills.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="khen_skills.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Accelerating Long-Horizon Planning with Affordance-Directed Dynamic Grounding of Abstract Strategies</div> <div class="author"> <a href="http://khen.io/" class="nonmember" rel="external nofollow noopener" target="_blank">Khen Elimelech</a>, <a href="/members/kingston">Zachary Kingston</a>, <a href="https://wbthomason.com/" class="nonmember" rel="external nofollow noopener" target="_blank">Wil Thomason</a>, <a href="https://www.cs.rice.edu/~vardi/" class="nonmember" rel="external nofollow noopener" target="_blank">Moshe Y. Vardi</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://khen.io/icra24appendix.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1109/ICRA57147.2024.10610486" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>Long-horizon task planning is important for robot autonomy, especially as a subroutine for frameworks such as Integrated Task and Motion Planning. However, task planning is computationally challenging and struggles to scale to realistic problem settings. We propose to accelerate task planning over an agent’s lifetime by integrating learned abstract strategies: a generalizable planning experience encoding introduced in earlier work. In this work, we contribute a practical approach to planning with strategies by introducing a novel formalism of planning in a skill-augmented domain. We also introduce and formulate the notion of a skill’s affordance, which indicates its predicted benefit to the solution, and use it to guide the planning and skill grounding processes. Together, our observations yield an affordance-directed, lazy-search planning algorithm, which can seamlessly compose strategies and actions to solve long-horizon planning problems. We evaluate our planner in an object rearrangement domain, where we demonstrate performance benefits relative to a state-of-the-art task planner.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">elimelech2024skills</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accelerating Long-Horizon Planning with Affordance-Directed Dynamic Grounding of Abstract Strategies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Elimelech, Khen and Kingston, Zachary and Thomason, Wil and Vardi, Moshe Y. and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12688--12695}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA57147.2024.10610486}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <a name="bayraktar2023" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#BB921F"> <a href="https://www.ieee-ras.org/publications/ra-l" rel="external nofollow noopener" target="_blank"> RA-L </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/larrt-480.webp 480w,/assets/img/publication_preview/larrt-800.webp 800w,/assets/img/publication_preview/larrt-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/larrt.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="larrt.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Solving Rearrangement Puzzles using Path Defragmentation in Factored State Spaces</div> <div class="author"> S. Bora Bayraktar, <a href="https://www.aorthey.com/" class="nonmember" rel="external nofollow noopener" target="_blank">Andreas Orthey</a>, <a href="/members/kingston">Zachary Kingston</a>, <a href="https://www.user.tu-berlin.de/mtoussai/" class="nonmember" rel="external nofollow noopener" target="_blank">Marc Toussaint</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2212.02955.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1109/LRA.2023.3282788" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>Rearrangement puzzles are variations of rearrangement problems in which the elements of a problem are potentially logically linked together. To efficiently solve such puzzles, we develop a motion planning approach based on a new state space that is logically factored, integrating the capabilities of the robot through factors of simultaneously manipulatable joints of an object. Based on this factored state space, we propose less-actions RRT (LA-RRT), a planner which optimizes for a low number of actions to solve a puzzle. At the core of our approach lies a new path defragmentation method, which rearranges and optimizes consecutive edges to minimize action cost. We solve six rearrangement scenarios with a Fetch robot, involving planar table puzzles and an escape room scenario. LA-RRT significantly outperforms the next best asymptotically-optimal planner by 4.01 to 6.58 times improvement in final action cost.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bayraktar2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Solving Rearrangement Puzzles using Path Defragmentation in Factored State Spaces}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bayraktar, S. Bora and Orthey, Andreas and Kingston, Zachary and Toussaint, Marc and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4529--4536}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LRA.2023.3282788}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <a name="lee2023physics" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#daaa00"> <a href="https://ieee-icra.org" rel="external nofollow noopener" target="_blank"> ICRA </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/physfeas-480.webp 480w,/assets/img/publication_preview/physfeas-800.webp 800w,/assets/img/publication_preview/physfeas-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/physfeas.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="physfeas.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Object Reconfiguration with Simulation-Derived Feasible Actions</div> <div class="author"> Yiyuan Lee, <a href="https://wbthomason.com/" class="nonmember" rel="external nofollow noopener" target="_blank">Wil Thomason</a>, <a href="/members/kingston">Zachary Kingston</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://kavrakilab.org/publications/lee2023-simulation-actions.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1109/ICRA48891.2023.10160377" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>3D object reconfiguration encompasses common robot manipulation tasks in which a set of objects must be moved through a series of physically feasible state changes into a desired final configuration. Object reconfiguration is challenging to solve in general, as it requires efficient reasoning about environment physics that determine action validity. This information is typically manually encoded in an explicit transition system. Constructing these explicit encodings is tedious and error-prone, and is often a bottleneck for planner use. In this work, we explore embedding a physics simulator within a motion planner to implicitly discover and specify the valid actions from any state, removing the need for manual specification of action semantics. Our experiments demonstrate that the resulting simulation-based planner can effectively produce physically valid rearrangement trajectories for a range of 3D object reconfiguration problems without requiring more than an environment description and start and goal arrangements.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lee2023physics</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Object Reconfiguration with Simulation-Derived Feasible Actions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lee, Yiyuan and Thomason, Wil and Kingston, Zachary and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8104--8111}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48891.2023.10160377}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <a name="quintero2023optimal" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#daaa00"> <a href="https://ieee-icra.org" rel="external nofollow noopener" target="_blank"> ICRA </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/optgrasp-480.webp 480w,/assets/img/publication_preview/optgrasp-800.webp 800w,/assets/img/publication_preview/optgrasp-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/optgrasp.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="optgrasp.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Optimal Grasps and Placements for Task and Motion Planning in Clutter</div> <div class="author"> <a href="https://carlosquinterop.github.io/" class="nonmember" rel="external nofollow noopener" target="_blank">Carlos Quintero-Peña</a>, <a href="/members/kingston">Zachary Kingston</a>, <a href="https://www.tianyangpan.com/" class="nonmember" rel="external nofollow noopener" target="_blank">Tianyang Pan</a>, <a href="https://www.rahulsho.me/" class="nonmember" rel="external nofollow noopener" target="_blank">Rahul Shome</a>, <a href="http://akyrillidis.github.io/about/" class="nonmember" rel="external nofollow noopener" target="_blank">Anastasios Kyrillidis</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://kavrakilab.org/publications/quintero2023-optimal-tmp.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1109/ICRA48891.2023.10161455" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>Many methods that solve robot planning problems, such as task and motion planners, employ discrete symbolic search to find sequences of valid symbolic actions that are grounded with motion planning. Much of the efficacy of these planners lies in this grounding—bad placement and grasp choices can lead to inefficient planning when a problem has many geometric constraints. Moreover, grounding methods such as naı̈ve sampling often fail to find appropriate values for these choices in the presence of clutter. Towards efficient task and motion planning, we present a novel optimization-based approach for grounding to solve cluttered problems that have many constraints that arise from geometry. Our approach finds an optimal grounding and can provide feedback to discrete search for more effective planning. We demonstrate our method against baseline methods in complex simulated environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">quintero2023optimal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimal Grasps and Placements for Task and Motion Planning in Clutter}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Quintero-Peña, Carlos and Kingston, Zachary and Pan, Tianyang and Shome, Rahul and Kyrillidis, Anastasios and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3707--3713}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48891.2023.10161455}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <a name="kingston2023tro" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#9C793E"> <a href="https://www.ieee-ras.org/publications/t-ro" rel="external nofollow noopener" target="_blank"> T-RO </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/r2_walking-480.webp 480w,/assets/img/publication_preview/r2_walking-800.webp 800w,/assets/img/publication_preview/r2_walking-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/r2_walking.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="r2_walking.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Scaling Multimodal Planning: Using Experience and Informing Discrete Search</div> <div class="author"> <a href="/members/kingston">Zachary Kingston</a> and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>IEEE Transactions on Robotics</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://kavrakilab.org/publications/kingston2022-scaling-mmp.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://player.vimeo.com/video/743110686?loop=1&amp;color=ffffff&amp;byline=0&amp;portrait=0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://doi.org/10.1109/TRO.2022.3197080" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>Robotic manipulation is inherently continuous, but typically has an underlying discrete structure, such as if an object is grasped. Many problems like these are multi-modal, such as pick-and-place tasks where every object grasp and placement is a mode. Multi-modal problems require finding a sequence of transitions between modes - for example, a particular sequence of object picks and placements. However, many multi-modal planners fail to scale when motion planning is difficult (e.g., in clutter) or the task has a long horizon (e.g., rearrangement). This work presents solutions for multi-modal scalability in both these areas. For motion planning, we present an experience-based planning framework ALEF which reuses experience from similar modes both online and from training data. For task satisfaction, we present a layered planning approach that uses a discrete lead to bias search towards useful mode transitions, informed by weights over mode transitions. Together, these contributions enable multi-modal planners to tackle complex manipulation tasks that were previously infeasible or inefficient, and provide significant improvements in scenes with high-dimensional robots.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kingston2023tro</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scaling Multimodal Planning: Using Experience and Informing Discrete Search}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kingston, Zachary and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Robotics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{128--146}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TRO.2022.3197080}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <a name="kingston2021" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#DA9200"> <a href="https://ieee-iros.org/" rel="external nofollow noopener" target="_blank"> IROS </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/alef-480.webp 480w,/assets/img/publication_preview/alef-800.webp 800w,/assets/img/publication_preview/alef-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/alef.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="alef.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Using Experience to Improve Constrained Planning on Foliations for Multi-Modal Problems</div> <div class="author"> <a href="/members/kingston">Zachary Kingston</a>, <a href="https://cchamzas.com/" class="nonmember" rel="external nofollow noopener" target="_blank">Constantinos Chamzas</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.kavrakilab.org/publications/kingston2021experience-foliations.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1109/IROS51168.2021.9636236" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>Many robotic manipulation problems are multi-modal—they consist of a discrete set of mode families (e.g., whether an object is grasped or placed) each with a continuum of parameters (e.g., where exactly an object is grasped). Core to these problems is solving single-mode motion plans, i.e., given a mode from a mode family (e.g., a specific grasp), find a feasible motion to transition to the next desired mode. Many planners for such problems have been proposed, but complex manipulation plans may require prohibitively long computation times due to the difficulty of solving these underlying single-mode problems. It has been shown that using experience from similar planning queries can significantly improve the efficiency of motion planning. However, even though modes from the same family are similar, they impose different constraints on the planning problem, and thus experience gained in one mode cannot be directly applied to another. We present a new experience-based framework, ALEF , for such multi-modal planning problems. ALEF learns using paths from single-mode problems from a mode family, and applies this experience to novel modes from the same family. We evaluate ALEF on a variety of challenging problems and show a significant improvement in the efficiency of sampling-based planners both in isolation and within a multi-modal manipulation planner.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kingston2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using Experience to Improve Constrained Planning on Foliations for Multi-Modal Problems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kingston, Zachary and Chamzas, Constantinos and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6922--6927}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS51168.2021.9636236}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <a name="wells2021icra" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#daaa00"> <a href="https://ieee-icra.org" rel="external nofollow noopener" target="_blank"> ICRA </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/finite_synth-480.webp 480w,/assets/img/publication_preview/finite_synth-800.webp 800w,/assets/img/publication_preview/finite_synth-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/finite_synth.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="finite_synth.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Finite Horizon Synthesis for Probabilistic Manipulation Domains</div> <div class="author"> <a href="https://andrewmw94.github.io/" class="nonmember" rel="external nofollow noopener" target="_blank">Andrew M. Wells</a>, <a href="/members/kingston">Zachary Kingston</a>, <a href="https://mortezalahijanian.com/" class="nonmember" rel="external nofollow noopener" target="_blank">Morteza Lahijanian</a>, <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a>, and <a href="https://www.cs.rice.edu/~vardi/" class="nonmember" rel="external nofollow noopener" target="_blank">Moshe Y. Vardi</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.kavrakilab.org/publications/wells2021-finite-horizon-synthesis.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1109/ICRA48506.2021.9561297" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>Robots have begun operating and collaborating with humans in industrial and social settings. This collaboration introduces challenges: the robot must plan while taking the human’s actions into account. In prior work, the problem was posed as a 2-player deterministic game, with a limited number of human moves. The limit on human moves is unintuitive, and in many settings determinism is undesirable. In this paper, we present a novel planning method for collaborative human-robot manipulation tasks via probabilistic synthesis. We introduce a probabilistic manipulation domain that captures the interaction by allowing for both robot and human actions with states that represent the configurations of the objects in the workspace. The task is specified using Linear Temporal Logic over finite traces (LTLf). We then transform our manipulation domain into a Markov Decision Process (MDP) and synthesize an optimal policy to satisfy the specification on this MDP. We present two novel contributions: a formalization of probabilistic manipulation domains allowing us to apply existing techniques and a comparison of different encodings of these domains. Our framework is validated on a physical UR5 robot.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wells2021icra</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Finite Horizon Synthesis for Probabilistic Manipulation Domains}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wells, Andrew M. and Kingston, Zachary and Lahijanian, Morteza and Kavraki, Lydia E. and Vardi, Moshe Y.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6336--6342}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48506.2021.9561297}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <a name="kingston2020leads" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#daaa00"> <a href="https://ieee-icra.org" rel="external nofollow noopener" target="_blank"> ICRA </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmp-480.webp 480w,/assets/img/publication_preview/mmp-800.webp 800w,/assets/img/publication_preview/mmp-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mmp.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmp.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Informing Multi-Modal Planning with Synergistic Discrete Leads</div> <div class="author"> <a href="/members/kingston">Zachary Kingston</a>, <a href="https://andrewmw94.github.io/" class="nonmember" rel="external nofollow noopener" target="_blank">Andrew M. Wells</a>, <a href="https://moll.ai/" class="nonmember" rel="external nofollow noopener" target="_blank">Mark Moll</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://kavrakilab.org/publications/kingston2020weighting-multi-modal-leads.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://player.vimeo.com/video/393316293?loop=1&amp;color=ffffff&amp;byline=0&amp;portrait=0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://doi.org/10.1109/ICRA40945.2020.9197545" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>Robotic manipulation problems are inherently continuous, but typically have underlying discrete structure, e.g., whether or not an object is grasped. This means many problems are multi-modal and in particular have a continuous infinity of modes. For example, in a pick-and-place manipulation domain, every grasp and placement of an object is a mode. Usually manipulation problems require the robot to transition into different modes, e.g., going from a mode with an object placed to another mode with the object grasped. To successfully find a manipulation plan, a planner must find a sequence of valid single-mode motions as well as valid transitions between these modes. Many manipulation planners have been proposed to solve tasks with multi-modal structure. However, these methods require mode-specific planners and fail to scale to very cluttered environments or to tasks that require long sequences of transitions. This paper presents a general layered planning approach to multi-modal planning that uses a discrete "lead" to bias search towards useful mode transitions. The difficulty of achieving specific mode transitions is captured online and used to bias search towards more promising sequences of modes. We demonstrate our planner on complex scenes and show that significant performance improvements are tied to both our discrete "lead" and our continuous representation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kingston2020leads</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Informing Multi-Modal Planning with Synergistic Discrete Leads}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kingston, Zachary and Wells, Andrew M. and Moll, Mark and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3199--3205}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA40945.2020.9197545}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <a name="dantam2018tmp" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#7D615D"> <a href="https://journals.sagepub.com/home/ijr" rel="external nofollow noopener" target="_blank"> IJRR </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/idtmp_deep-480.webp 480w,/assets/img/publication_preview/idtmp_deep-800.webp 800w,/assets/img/publication_preview/idtmp_deep-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/idtmp_deep.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="idtmp_deep.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">An Incremental Constraint-Based Framework for Task and Motion Planning</div> <div class="author"> <a href="http://www.neil.dantam.name/" class="nonmember" rel="external nofollow noopener" target="_blank">Neil T. Dantam</a>, <a href="/members/kingston">Zachary Kingston</a>, <a href="https://www.cs.utexas.edu/~swarat/" class="nonmember" rel="external nofollow noopener" target="_blank">Swarat Chaudhuri</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>The International Journal of Robotics Research, Special Issue on the 2016 Robotics: Science and Systems Conference</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://kavrakilab.org/publications/dantam2018incremental-tmp.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="http://tmkit.kavrakilab.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://doi.org/10.1177/0278364918761570" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>We present a new algorithm for task and motion planning (TMP) and discuss the requirements and abstrations necessary to obtain robust solutions for TMP in general. Our Iteratively Deepened Task and Motion Planning (IDTMP) method is probabilistically-complete and offers improved performance and generality compared to a similar, state-of-the-art, probabilistically-complete planner. The key idea of IDTMP is to leverage incremental constraint solving to efficiently add and remove constraints on motion feasibility at the task level. We validate IDTMP on a physical manipulator and evaluate scalability on scenarios with many objects and long plans, showing order-of-magnitude gains compared to the benchmark planner and a four-times self-comparison speedup from our extensions. Finally, in addition to describing a new method for TMP and its implementation on a physical robot, we also put forward requirements and abstractions for the development of similar planners in the future.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">dantam2018tmp</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Incremental Constraint-Based Framework for Task and Motion Planning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dantam, Neil T. and Kingston, Zachary and Chaudhuri, Swarat and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The International Journal of Robotics Research, Special Issue on the 2016 Robotics: Science and Systems Conference}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1134--1151}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1177/0278364918761570}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <a name="dantam2016tmp" class="anchor"></a> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#DA7900"> <a href="https://roboticsconference.org/" rel="external nofollow noopener" target="_blank"> RSS </a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/idtmp-480.webp 480w,/assets/img/publication_preview/idtmp-800.webp 800w,/assets/img/publication_preview/idtmp-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/idtmp.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="idtmp.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="col-sm-10"> <div class="title">Incremental Task and Motion Planning: A Constraint-Based Approach</div> <div class="author"> <a href="http://www.neil.dantam.name/" class="nonmember" rel="external nofollow noopener" target="_blank">Neil T. Dantam</a>, <a href="/members/kingston">Zachary Kingston</a>, <a href="https://www.cs.utexas.edu/~swarat/" class="nonmember" rel="external nofollow noopener" target="_blank">Swarat Chaudhuri</a>, and <a href="https://profiles.rice.edu/faculty/lydia-e-kavraki" class="nonmember" rel="external nofollow noopener" target="_blank">Lydia E. Kavraki</a> </div> <div class="periodical"> <em>In Robotics: Science and Systems</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://kavrakilab.org/publications/dantam2016tmp.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/QHCuD0tOdfY?si=dCfjMJHFuLzxMNom" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://youtu.be/9EcOJ8mF5JE?si=9uhdZpqSHZh25oIS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Talk</a> <a href="http://tmkit.kavrakilab.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://doi.org/10.15607/RSS.2016.XII.002" class="ai ai-doi" rel="external nofollow noopener" target="_blank"></a> </div> <div class="abstract hidden"> <p>We present a new algorithm for task and motion planning (TMP) and discuss the requirements and abstractions necessary to obtain robust solutions for TMP in general. Our Iteratively Deepened Task and Motion Planning (IDTMP) method is probabilistically-complete and offers improved performance and generality compared to a similar, state-of-the-art, probabilistically-complete planner. The key idea of IDTMP is to leverage incremental constraint solving to efficiently add and remove constraints on motion feasibility at the task level. We validate IDTMP on a physical manipulator and evaluate scalability on scenarios with many objects and long plans, showing order-of-magnitude gains compared to the benchmark planner and a four-times self-comparison speedup from our extensions. Finally, in addition to describing a new method for TMP and its implementation on a physical robot, we also put forward requirements and abstractions for the development of similar planners in the future.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dantam2016tmp</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Incremental Task and Motion Planning: A Constraint-Based Approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dantam, Neil T. and Kingston, Zachary and Chaudhuri, Swarat and Kavraki, Lydia E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.15607/RSS.2016.XII.002}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Ann Arbor, MI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> CoMMA Lab @ <a href="https://www.purdue.edu/" rel="external nofollow noopener" target="_blank">Purdue University</a>, <a href="https://www.cs.purdue.edu/" rel="external nofollow noopener" target="_blank">Department of Computer Science</a>. © Copyright 2026. Last updated: January 18, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>